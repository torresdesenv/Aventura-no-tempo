# ğŸ—£ï¸ LipRead Translator

**Aplicativo de Leitura Labial e TraduÃ§Ã£o em Tempo Real**

Um aplicativo React Native para iOS e Android que utiliza InteligÃªncia Artificial para fazer leitura labial de vÃ­deos e cÃ¢mera ao vivo, traduzindo para mÃºltiplos idiomas com sÃ­ntese de voz.

---

## âœ¨ Funcionalidades

### ğŸ“¹ Principais
- **Upload de VÃ­deo**: Carregue vÃ­deos e selecione a pessoa para leitura labial
- **CÃ¢mera em Tempo Real**: Leitura labial ao vivo usando a cÃ¢mera do dispositivo
- **DetecÃ§Ã£o Facial**: IdentificaÃ§Ã£o automÃ¡tica de mÃºltiplas pessoas no vÃ­deo
- **TraduÃ§Ã£o AutomÃ¡tica**: Suporte para 9+ idiomas
- **Text-to-Speech**: Voz feminina e masculina com personalizaÃ§Ã£o

### ğŸ¯ Recursos AvanÃ§ados
- SeleÃ§Ã£o de idioma de origem e destino
- Ajuste de velocidade e tom da voz
- Indicador de confianÃ§a da leitura labial
- Interface limpa e intuitiva
- Modo claro/escuro (planejado)
- HistÃ³rico de traduÃ§Ãµes (planejado)

---

## ğŸš€ Tecnologias Utilizadas

- **React Native** 0.72.3
- **Expo** 49.0.0
- **React Navigation** 6.x
- **TensorFlow.js** (para ML)
- **Expo Camera** (cÃ¢mera)
- **Expo Speech** (TTS)
- **Expo Image Picker** (seleÃ§Ã£o de vÃ­deos)
- **AsyncStorage** (persistÃªncia)

---

## ğŸ“‹ PrÃ©-requisitos

Antes de comeÃ§ar, certifique-se de ter instalado:

- **Node.js** >= 16.x
- **npm** ou **yarn**
- **Expo CLI**: `npm install -g expo-cli`
- **Git**

### Para desenvolvimento iOS:
- **macOS** com Xcode instalado
- **CocoaPods**: `sudo gem install cocoapods`

### Para desenvolvimento Android:
- **Android Studio** com SDK configurado
- **Java JDK** >= 11

---

## ğŸ”§ InstalaÃ§Ã£o

### 1. Clone o repositÃ³rio

```bash
git clone https://github.com/seu-usuario/lip-reading-app.git
cd lip-reading-app
```

### 2. Instale as dependÃªncias

```bash
npm install
# ou
yarn install
```

### 3. Instale dependÃªncias do iOS (somente macOS)

```bash
cd ios
pod install
cd ..
```

---

## ğŸ® Como Executar

### Modo de Desenvolvimento com Expo

```bash
# Iniciar servidor de desenvolvimento
npm start
# ou
expo start
```

Isso abrirÃ¡ o Expo Dev Tools no navegador. A partir daÃ­ vocÃª pode:

- Pressionar `i` para abrir no simulador iOS
- Pressionar `a` para abrir no emulador Android
- Escanear o QR code com o app Expo Go no seu dispositivo fÃ­sico

### iOS (Simulador)

```bash
npm run ios
# ou
expo start --ios
```

### Android (Emulador)

```bash
npm run android
# ou
expo start --android
```

### Web (Preview)

```bash
npm run web
# ou
expo start --web
```

---

## ğŸ“± Estrutura do Projeto

```
lip-reading-app/
â”œâ”€â”€ App.js                      # Componente principal
â”œâ”€â”€ app.json                    # ConfiguraÃ§Ã£o do Expo
â”œâ”€â”€ package.json                # DependÃªncias
â”œâ”€â”€ babel.config.js             # ConfiguraÃ§Ã£o Babel
â”œâ”€â”€ metro.config.js             # ConfiguraÃ§Ã£o Metro
â”œâ”€â”€ assets/                     # Imagens e Ã­cones
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/            # Componentes reutilizÃ¡veis
â”‚   â”‚   â”œâ”€â”€ Button.js
â”‚   â”‚   â”œâ”€â”€ Card.js
â”‚   â”‚   â”œâ”€â”€ Header.js
â”‚   â”‚   â”œâ”€â”€ VoiceSelector.js
â”‚   â”‚   â”œâ”€â”€ LanguageSelector.js
â”‚   â”‚   â””â”€â”€ TranslationDisplay.js
â”‚   â”œâ”€â”€ screens/               # Telas do app
â”‚   â”‚   â”œâ”€â”€ HomeScreen.js
â”‚   â”‚   â”œâ”€â”€ UploadVideoScreen.js
â”‚   â”‚   â”œâ”€â”€ RealtimeCameraScreen.js
â”‚   â”‚   â””â”€â”€ SettingsScreen.js
â”‚   â”œâ”€â”€ navigation/            # NavegaÃ§Ã£o
â”‚   â”‚   â””â”€â”€ AppNavigator.js
â”‚   â”œâ”€â”€ services/              # ServiÃ§os e APIs
â”‚   â”‚   â”œâ”€â”€ LipReadingService.js
â”‚   â”‚   â”œâ”€â”€ TranslationService.js
â”‚   â”‚   â”œâ”€â”€ TTSService.js
â”‚   â”‚   â””â”€â”€ FaceDetectionService.js
â”‚   â”œâ”€â”€ utils/                 # UtilitÃ¡rios
â”‚   â”‚   â””â”€â”€ constants.js
â”‚   â””â”€â”€ styles/                # Estilos globais
â”‚       â”œâ”€â”€ colors.js
â”‚       â””â”€â”€ globalStyles.js
â””â”€â”€ README.md
```

---

## ğŸ”‘ ConfiguraÃ§Ã£o de APIs

Este projeto utiliza serviÃ§os de ML e traduÃ§Ã£o. Para produÃ§Ã£o, vocÃª precisarÃ¡ configurar:

### 1. Google Translate API

Edite `src/services/TranslationService.js`:

```javascript
this.apiKey = 'SUA_CHAVE_API_GOOGLE_TRANSLATE';
```

[Obtenha uma chave aqui](https://cloud.google.com/translate/docs/setup)

### 2. Modelo de Leitura Labial

O serviÃ§o atual usa simulaÃ§Ã£o. Para produÃ§Ã£o, integre com:

- **LipNet** - Modelo de deep learning para leitura labial
- **Google Cloud Video Intelligence API**
- **Azure Video Analyzer**
- **Modelo personalizado treinado**

Edite `src/services/LipReadingService.js` para carregar seu modelo:

```javascript
this.model = await tf.loadLayersModel('path/to/your/model.json');
```

### 3. DetecÃ§Ã£o Facial

Configure detecÃ§Ã£o facial real editando `src/services/FaceDetectionService.js`:

```javascript
import * as FaceDetector from 'expo-face-detector';
// Ou use MediaPipe, ML Kit, etc.
```

---

## ğŸ¨ PersonalizaÃ§Ã£o

### Cores

Edite `src/styles/colors.js` para alterar o tema:

```javascript
export const colors = {
  primary: '#6366f1',      // Cor principal
  secondary: '#ec4899',    // Cor secundÃ¡ria
  accent: '#10b981',       // Cor de destaque
  // ...
};
```

### Idiomas Suportados

Adicione mais idiomas em `src/utils/constants.js`:

```javascript
export const LANGUAGES = [
  { code: 'pt', name: 'PortuguÃªs', flag: 'ğŸ‡§ğŸ‡·' },
  { code: 'en', name: 'English', flag: 'ğŸ‡ºğŸ‡¸' },
  // Adicione mais aqui
];
```

---

## ğŸ“¦ Build para ProduÃ§Ã£o

### Build Android (APK)

```bash
# Build de desenvolvimento
expo build:android -t apk

# Build de produÃ§Ã£o (AAB para Play Store)
expo build:android -t app-bundle
```

### Build iOS (IPA)

```bash
# Build para dispositivos
expo build:ios -t archive

# Build para simulador
expo build:ios -t simulator
```

### EAS Build (Recomendado)

```bash
# Instalar EAS CLI
npm install -g eas-cli

# Login no Expo
eas login

# Configurar projeto
eas build:configure

# Build Android
eas build --platform android

# Build iOS
eas build --platform ios
```

---

## ğŸ§ª Testes

```bash
# Executar testes
npm test

# Testes com coverage
npm run test:coverage

# Testes em watch mode
npm run test:watch
```

---

## ğŸ› SoluÃ§Ã£o de Problemas

### Erro: "Unable to resolve module"

```bash
# Limpar cache
npm start -- --reset-cache
# ou
expo start -c
```

### Erro de permissÃµes (iOS)

Certifique-se de ter as permissÃµes em `app.json`:

```json
"ios": {
  "infoPlist": {
    "NSCameraUsageDescription": "Precisamos de acesso Ã  cÃ¢mera...",
    "NSMicrophoneUsageDescription": "Precisamos de acesso ao microfone..."
  }
}
```

### Erro de permissÃµes (Android)

Verifique `app.json`:

```json
"android": {
  "permissions": [
    "CAMERA",
    "RECORD_AUDIO",
    "READ_EXTERNAL_STORAGE"
  ]
}
```

### Problemas com Expo Go

Se o app nÃ£o funcionar no Expo Go devido a dependÃªncias nativas, use:

```bash
expo prebuild
npm run ios
# ou
npm run android
```

---

## ğŸš§ Roadmap

### VersÃ£o 1.1 (Planejado)
- [ ] HistÃ³rico de traduÃ§Ãµes
- [ ] Exportar transcriÃ§Ãµes (PDF, TXT)
- [ ] Modo offline
- [ ] Suporte a mais idiomas
- [ ] Tema escuro/claro

### VersÃ£o 1.2 (Planejado)
- [ ] Modo reuniÃ£o (mÃºltiplas pessoas)
- [ ] IntegraÃ§Ã£o com Zoom/Meet
- [ ] Compartilhamento de traduÃ§Ãµes
- [ ] Nuvem de sincronizaÃ§Ã£o

### VersÃ£o 2.0 (Futuro)
- [ ] Modelo de ML prÃ³prio treinado
- [ ] Processamento 100% offline
- [ ] Reconhecimento de gestos
- [ ] Suporte a linguagem de sinais

---

## ğŸ¤ Contribuindo

ContribuiÃ§Ãµes sÃ£o bem-vindas! Por favor:

1. FaÃ§a um fork do projeto
2. Crie uma branch para sua feature (`git checkout -b feature/MinhaFeature`)
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova feature'`)
4. Push para a branch (`git push origin feature/MinhaFeature`)
5. Abra um Pull Request

---

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ sob a licenÃ§a MIT. Veja o arquivo [LICENSE](LICENSE) para mais detalhes.

---

## ğŸ‘¥ Autores

- **Seu Nome** - *Desenvolvimento inicial* - [GitHub](https://github.com/seu-usuario)

---

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/seu-usuario/lip-reading-app/issues)
- **Email**: seu-email@exemplo.com
- **Discord**: [Link do servidor](https://discord.gg/seu-servidor)

---

## ğŸ™ Agradecimentos

- Expo Team pelo framework incrÃ­vel
- Comunidade React Native
- Pesquisadores de ML em lip reading
- Todos os contribuidores

---

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o
- [React Native Docs](https://reactnative.dev/)
- [Expo Docs](https://docs.expo.dev/)
- [React Navigation](https://reactnavigation.org/)
- [TensorFlow.js](https://www.tensorflow.org/js)

### Tutoriais de Lip Reading
- [LipNet Paper](https://arxiv.org/abs/1611.01599)
- [Lip Reading Tutorial](https://github.com/topics/lip-reading)
- [MediaPipe Face Mesh](https://google.github.io/mediapipe/solutions/face_mesh.html)

### APIs Ãšteis
- [Google Cloud Translation](https://cloud.google.com/translate)
- [Azure Translator](https://azure.microsoft.com/en-us/services/cognitive-services/translator/)
- [Google Cloud Video Intelligence](https://cloud.google.com/video-intelligence)

---

## ğŸ“Š Status do Projeto

![Version](https://img.shields.io/badge/version-1.0.0-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![React Native](https://img.shields.io/badge/React%20Native-0.72.3-blue.svg)
![Expo](https://img.shields.io/badge/Expo-49.0.0-black.svg)

---

**Desenvolvido com â¤ï¸ para tornar a comunicaÃ§Ã£o mais acessÃ­vel**
